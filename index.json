[{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Report “Generative AI with Amazon Bedrock” Event Objectives Gain an overview of Generative AI and its applications on AWS. Introduce Amazon Bedrock and foundation models. Practice Prompt Engineering and RAG (Retrieval Augmented Generation) techniques. Explore pre-trained AI services on AWS. Learn how to build agentic AI with Amazon Bedrock AgentCore. Speakers Danh Hoang Hieu Nghi – AI Cloud Dinh Le Hoang Anh – AI Engineer Lam Tuan Kiet – Sr DevOps Engineer Key Content Foundation Model and Amazon Bedrock Why Foundation Models? Traditional ML models: limited generalization Foundation Models: better generalization across multiple domains Amazon Bedrock: platform providing foundation models as a service Use case: chatbot in banking Prompt Engineering What is a prompt? → Input to AI → Output result Prompt Engineering: techniques to optimize AI input for better output Techniques: Zero-shot prompting Few-shot prompting Chain of Thought (CoT) Retrieval Augmented Generation (RAG) What is RAG? Retrieval → Augmented → Generation RAG use cases Embeddings: converting language into high-dimensional vector space Amazon Titan Embedding Data ingestion \u0026amp; RAG in action Other Pre-trained AI Services Amazon Rekognition Amazon Translate Amazon Textract Amazon Transcribe Amazon Polly Amazon Comprehend Amazon Kendra Amazon Lookout Family Amazon Personalize Pipecat framework (similar to ETL pipeline) Amazon Bedrock AgentCore Evolution into Agentic AI Framework for building agents from prototype → production Amazon Bedrock AgentCore includes: Runtime, Memory, Identity, Gateway Code interpreter Browser tool Observations Key Takeaways Thinking \u0026amp; Technical Insights Understand the role of foundation models and how to deploy them on Bedrock Practice Prompt Engineering to optimize AI outputs Apply RAG to improve information retrieval Recognize AWS pre-trained AI services and how to integrate them Hands-on Experience Create AI agent prototypes using Bedrock AgentCore Manage agent data, memory, and runtime effectively Use code interpreter and browser tool to enhance agent capabilities Application to Work Design intelligent chatbots for banking applications Use embeddings \u0026amp; RAG for faster information retrieval Integrate AWS AI services to accelerate application development Lessons Learned Prompting and RAG are crucial for optimizing GenAI solutions Amazon Bedrock AgentCore bridges prototype → production effectively Pre-trained AI services help reduce deployment time and increase efficiency "},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/1-worklog/1.6-week6/","title":"Worklog Week 6","tags":[],"description":"","content":"Week 6 Objectives Review fundamental Database Concepts. Learn how Amazon RDS and Amazon Aurora manage relational databases. Improve query performance using Amazon Redshift \u0026amp; ElastiCache. Perform data migration using AWS Schema Conversion. Build a Data Lake on AWS with real datasets. Work with Amazon DynamoDB: table design \u0026amp; query optimization. Analyze cost \u0026amp; performance using AWS Glue and Amazon Athena. Create visual dashboards with Amazon QuickSight. Tasks Completed During the Week Day Tasks Start Date Completion Date Reference 2 - Overview of Database Concepts - Studied Amazon RDS \u0026amp; Aurora 13/10/2025 13/10/2025 CloudJourney 3 - Designed database schema \u0026amp; migration with Schema Conversion - Improved query performance using Redshift 14/10/2025 14/10/2025 CloudJourney 4 - Learned ElastiCache for application performance - Designed a basic Data Lake on AWS 15/10/2025 15/10/2025 CloudJourney 5 - Worked with Amazon DynamoDB - Practiced query optimization \u0026amp; throughput tuning 16/10/2025 16/10/2025 CloudJourney 6 - Data analysis using AWS Glue \u0026amp; Athena - Created cost analysis tables 17/10/2025 17/10/2025 CloudJourney 7 - Built dashboards with QuickSight - Developed a real-world AWS Data Lake 18/10/2025 18/10/2025 CloudJourney Week 6 Achievements: Understood the differences between RDS, Aurora, DynamoDB, and Redshift. Applied schema design and data migration using Schema Conversion. Built a basic Data Lake using real datasets. Used ElastiCache to reduce latency and improve performance. Wrote queries with Athena and analyzed AWS cost data. Built an interactive dashboard using Amazon QuickSight. Reflection Difficulties faced:\nChoosing the appropriate database type for each business use case DynamoDB requires optimized partition keys \u0026amp; throughput settings Redshift performance drops when datasets are fragmented Glue \u0026amp; Athena require structured data formats QuickSight has limitations in the free tier End of Week 6 Report\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives Explore AWS security services in depth. Understand the Shared Responsibility Model. Learn access management using IAM, AWS Identity Center, and AWS Organizations. Implement authentication \u0026amp; authorization with Amazon Cognito. Manage encryption keys and secure data using AWS KMS. Enable and monitor AWS Security Hub, combined with cost optimization using Lambda. Manage resource access with Tags, Resource Groups, and Permission Boundary. Tasks Completed This Week Day Task Start Date Completion Date References 2 - Study the Shared Responsibility Model - Manage users \u0026amp; access with IAM 06/10/2025 06/10/2025 CloudJourney 3 - Implement user authentication with Amazon Cognito - Manage multiple AWS accounts using AWS Organizations 07/10/2025 07/10/2025 CloudJourney 4 - Configure AWS Identity Center - Encrypt data \u0026amp; rotate keys using AWS KMS 08/10/2025 08/10/2025 CloudJourney 5 - Enable AWS Security Hub - Analyze security findings \u0026amp; alerts 09/10/2025 09/10/2025 CloudJourney 6 - Optimize EC2 costs using Lambda Scheduler - Manage resources with Tags \u0026amp; Resource Groups 10/10/2025 10/10/2025 CloudJourney 7 - IAM Hands-on: • Grant access to EC2 using Tags • Set up Permission Boundary • Create IAM Role with Conditions • Allow apps to securely access AWS services 11/10/2025 11/10/2025 CloudJourney Week 5 Achievements: Understood the Shared Responsibility Model between AWS and customers. Learned IAM components: User, Group, Role, Policy, and policy conditions. Built User Pool \u0026amp; Identity Pool using Amazon Cognito for authentication. Secured sensitive data using AWS KMS with key rotation. Enabled AWS Security Hub and analyzed security findings based on best practices. Reduced EC2 costs automatically using Lambda Scheduler. Organized resources with Tags + Resource Groups, and restricted access using Permission Boundary. Implemented IAM Roles to grant cross-service access securely on AWS. Evaluation \u0026amp; Challenges Challenges Solutions IAM has many policy types, confusing between Inline \u0026amp; Managed Policy Created standardized documentation \u0026amp; policy templates for different use cases AWS Organizations requires clear OU structure Designed an OU architecture before implementation Cognito setup requires knowledge of OAuth2 \u0026amp; JWT Studied token flows and JWT exchange mechanisms Security Hub returns many alerts and findings Classified alerts by priority before solving Lambda cost optimization requires correct IAM Role Created a dedicated IAM Role for Lambda \u0026amp; tested before deployment End of Week 5 Report\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Learn about AWS storage services: S3, Storage Gateway, FSx. Practice creating S3 Buckets and deploying infrastructure using IaC. Implement AWS Backup for EC2 and EBS resources. Perform VM Import/Export from on-premises to AWS. Work with Amazon FSx for Windows File Server. Improve skills in cloud storage management and backup strategies. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 2 - Create S3 Bucket - Learn about S3 storage classes 29/09/2025 29/09/2025 CloudJourney 3 - Deploy infrastructure using IaC - Use templates to build S3 and other resources 30/09/2025 30/09/2025 CloudJourney 4 - Implement AWS Backup (Backup Plan \u0026amp; Vault) - Assign backup to EC2/EBS 01/10/2025 01/10/2025 CloudJourney 5 - Practice VM Import/Export from local machine to AWS - Learn the workloads migration process 02/10/2025 02/10/2025 CloudJourney 6 - Deploy AWS Storage Gateway (File Gateway) - Map local file storage to S3 03/10/2025 03/10/2025 CloudJourney Week 4 Achievements: Successfully created and managed S3 Buckets using both console and CLI. Understood how storage classes and lifecycle rules work in S3. Deployed cloud infrastructure automatically using IaC templates. Implemented AWS Backup for EC2/EBS with scheduled backup plans. Performed VM Import/Export from on-premises to AWS. Configured Storage Gateway to link local file system to S3. Deployed Amazon FSx for Windows File Server and configured SMB file sharing. Gained real-world insight into enterprise storage and backup solutions. Reflection Difficulties faced:\nAWS Backup required strict IAM permissions to work properly. Storage Gateway setup required correct networking and DNS configuration. VM Import/Export was time-consuming and required proper disk formatting. Amazon FSx setup involved complex Active Directory and SMB configuration. End of Week 4 Report\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn about Hybrid DNS on AWS and Route 53 Resolver. Practice creating Key Pair, configuring Security Group, and connecting to RDGW. Understand DNS configuration within a VPC. Study VPC Peering and AWS Transit Gateway. Practice creating, configuring, and managing EC2 instances, Elastic Block Store, and Auto Scaling. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 2 - Learn about Hybrid DNS with Route 53 Resolver 22/09/2025 22/09/2025 CloudJourney 3 - Create Key Pair for EC2 - Configure Security Group - Connect to Remote Desktop Gateway (RDGW) 23/09/2025 23/09/2025 CloudJourney 4 - Configure DNS within VPC 24/09/2025 24/09/2025 CloudJourney 5 - Set up VPC Peering - Create Peering Connection - Configure Route Table and Cross-Peer DNS 25/09/2025 25/09/2025 CloudJourney 6 - Learn about AWS Transit Gateway - Practice creating EC2 instance, AMI, EBS, Instance Store, User Data, Metadata, Auto Scaling 26/09/2025 26/09/2025 CloudJourney Week 3 Achievements: Configured Hybrid DNS using Route 53 Resolver. Successfully created Key Pair, Security Group, and connected to RDGW. Understood and configured DNS in VPC and verified its operation. Set up VPC Peering and configured route table with cross-peer DNS. Learned the concept and deployed AWS Transit Gateway. Created and managed EC2 instances, AMI, EBS, Instance Store, User Data, Metadata, and Auto Scaling. Enhanced skills in network management, EC2, and secure connectivity on AWS. Reflection Difficulties faced:\nDNS configuration is complex with multiple steps and prone to errors. VPC Peering and Transit Gateway setup requires precise route table configuration. Practicing EC2 Auto Scaling and User Data takes time to fully understand the workflow. End of Week 3 Report\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Learn in-depth about AWS Virtual Private Cloud (VPC) and related features. Study network security, VPN, DirectConnect, and LoadBalancer. Practice creating Subnets, Route tables, Internet Gateway, NAT Gateway, Security Groups, Network ACLs, and connecting EC2 instances. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about AWS Virtual Private Cloud (VPC) - Study VPC security and Multi-VPC features 15/09/2025 15/09/2025 CloudJourney 3 - Study VPN site-to-site, DirectConnect, LoadBalancer - Introduction to VPC network architecture 16/09/2025 16/09/2025 CloudJourney 4 - Create and configure Subnets - Set up Route Tables - Configure Internet Gateway (IGW) - Set up NAT Gateway 17/09/2025 17/09/2025 CloudJourney 5 - Configure Security Groups - Configure Network ACLs - Review and analyze VPC Resource Map 18/09/2025 18/09/2025 CloudJourney 6 - Practice creating Subnet, Internet Gateway, Route Table for outbound internet - Create Security Groups - Test EC2 connection - Connect EC2 via Connect Endpoint 19/09/2025 19/09/2025 CloudJourney Week 2 Achievements: Understanding of AWS VPC and networking features: Subnet, Route Table, Internet Gateway, NAT Gateway, Security Groups, Network ACLs. Learned how to configure Multi-VPC, VPC security, VPN site-to-site, DirectConnect, and LoadBalancer. Successfully practiced creating AWS VPC components and tested EC2 connectivity. Able to manage AWS resources using both Management Console and CLI simultaneously. Understood the workflow for secure network deployment, outbound internet routing, Security Group configuration, and EC2 connection testing. Improved practical skills for network setup, troubleshooting, and access control on AWS. Reflection Difficulties faced:\nMany new concepts CLI commands were complex VPC concepts still abstract End of Week 2 Report\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Building a Resilient \u0026amp; Secure Game Backend with Amazon CloudFront By Serge Poueme \u0026amp; Adam Hatfield – August 21, 2025\nIn modern gaming, milliseconds can determine victory or defeat. Games have evolved from standalone experiences into always-on, live services, requiring real-time multiplayer interactions, complex social features, and frequent updates. Therefore, games need a fast, reliable, and secure backend to deliver a seamless global experience.\nThis blog explores how Amazon CloudFront enhances performance, availability, and security for game backends. CloudFront, widely known as a Content Delivery Network (CDN), distributes static assets via AWS Edge Network but can also act as a distributed single entry point for backend services using Application Load Balancers (ALBs) or Amazon API Gateway, isolating clients and reducing operational overhead through caching and traffic management.\nTypical Game Backend Architecture Game backends on AWS often use ALB or NLB to route traffic to core services, typically deployed on:\nAmazon EC2 Auto Scaling Groups AWS Lambda Container services (ECS/EKS) DDoS protection should be enabled as a best practice.\nSome studios also use Amazon API Gateway to handle HTTP/HTTPS requests and serverless endpoints.\nChallenges when moving to multiplayer/live-service models: Core services in a single region → higher latency for distant players. Public endpoints (ALB/API Gateway) → exposed to DDoS or MITM attacks. Amazon CloudFront What is CloudFront? Amazon CloudFront is a CDN that improves performance, scalability, and security for web applications by routing user requests through AWS Global Network to optimal edge locations.\nCloudFront can:\nServe content from applications in private VPC subnets using ALB or EC2 as origin. Integrate with Amazon API Gateway to protect and accelerate REST APIs through caching, request filtering, and reduced latency. Key Features for Game Studios CloudFront provides:\nEdge caching for static assets (patches, images, game data). Persistent TCP connections \u0026amp; optimized routing for dynamic requests (authentication, matchmaking). Native integration with AWS Shield \u0026amp; AWS WAF to block malicious traffic at the edge. CloudFront Functions to execute lightweight logic (authentication, request validation) at the edge. Modernizing Game Backends with CloudFront CloudFront enables:\nReduced global latency via edge network. A routing layer for backend APIs. Path-based routing \u0026amp; edge logic through CloudFront Functions and Lambda@Edge. Edge logic execution → lower latency \u0026amp; increased security. Common use cases: Use Case AWS Services Used Matchmaking CloudFront Functions + Lambda@Edge Player Authentication API Gateway + CloudFront Functions Content Moderation (UGC) AWS WAF rule sets + logging Amazon CloudFront WebSocket Support CloudFront natively supports WebSocket, suitable for real-time interactions like:\nMatchmaking lobbies Leaderboards In-game chat systems CloudFront also:\nHandles HTTP → WebSocket upgrades Maintains long-lived TCP connections at edge Automatically manages keep-alive, timeout, failover Integrates with CloudWatch \u0026amp; Route 53 for monitoring and routing AWS WAF \u0026amp; AWS Shield Integration Combining AWS WAF and AWS Shield Advanced provides:\nEarly DDoS mitigation at edge locations Managed security rule groups Cost protection against traffic spikes Multi-layered OSI protection Real-time alerts \u0026amp; AWS DDoS Response Team (DRT) support Operational Excellence for Global Game Architectures Using:\nAmazon CloudWatch AWS X-Ray VPC Flow Logs → Enables monitoring player behavior and anomaly detection.\nThe backend can auto-scale when traffic surges:\nCloudWatch Alarm → Auto Scaling → SNS Notification (optional) "},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Thanh Son\nPhone Number: 0862646970\nEmail: sonntse183379@fpt.edu.vn\nUniversity: FPT UNIVERSITY\nMajor: Information Technology\nStudent ID: SE183379\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 09/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/5-workshop/5.1-workshop-overview/","title":"Overview","tags":[],"description":"","content":"Workshop Overview In this workshop, you will build a complete AI Chatbot using serverless architecture on AWS. The chatbot uses Claude Haiku 4.5 deployed entirely serverless with low costs and automatic scaling capabilities.\nModules Module 1: Setup Amazon Bedrock Module 2: Create Lambda Function Module 3: Configure API Gateway Module 4: Deploy Frontend to S3 Module 5: Testing \u0026amp; Debugging\nArchitecture Overview AWS Services \u0026amp; Their Roles in the Workshop 1. Amazon Bedrock — AI Brain Process and generate responses from user messages. Uses Claude Haiku 4.5 (Messages API). Features: Inference Profiles Conversation history management Temperature control 2. AWS Lambda — Backend Logic Role:\nReceive requests from API Gateway Validate input, format prompt Call Bedrock and return response Features:\nNode.js 24 runtime Environment variables (MODEL_ID, ALLOWED_ORIGIN) CloudWatch Logs Input validation + error handling Retry logic CORS handling 3. Amazon API Gateway — API Endpoint Role:\nPublic REST API for browsers CORS management Route POST /chat to Lambda Throttling to prevent abuse Features:\nREST API or HTTP API CORS configuration Lambda Proxy Integration OPTIONS method for CORS preflight 4. Amazon S3 — Frontend Hosting Static Website Hosting Host HTML/CSS/JS Allow users to access chatbot via browser 5. Amazon CloudWatch — Monitoring \u0026amp; Debugging Collect logs from Lambda Debug request/response \u0026amp; errors Workshop Learning Objectives After the workshop, you will:\nUnderstand serverless architecture and how services interact Deploy a production-ready chatbot with Claude Haiku 4.5 Integrate Bedrock API with Lambda Configure API Gateway Host static website on S3 Debug using CloudWatch Logs Optimize serverless costs "},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/5-workshop/5.3-setup-amazon-bedrock/5.3.1-check-model/","title":"Setup Amazon Bedrock","tags":[],"description":"","content":"1. Go to Amazon Bedrock 2. Click on Model Catalog to find Claude Haiku 4.5 3. Click on Claude Haiku 4.5 and scroll down to find the Model ID 4. You can click \u0026ldquo;open in playground\u0026rdquo; to test the model Note: Currently you no longer need to go to model access to request model access. You can directly use the API of a model in Amazon Bedrock with an IAM account that has AWS marketplace permission once you call the model once to activate it References: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-claude.html https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 09/08/2025 09/08/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + Practice Lab01 Create AWS account + Learn VPC Security \u0026amp; VPC features 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn AWS Console \u0026amp; AWS CLI - Practice: + Install \u0026amp; configure AWS CLI + Account setup and cost management strategies 09/10/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS - SSH to EC2 - Learn Elastic IP - Monitoring \u0026amp; Observability 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements Understood AWS basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console:\nService navigation Billing \u0026amp; cost checking Installed \u0026amp; configured AWS CLI:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic commands:\nCheck account info Retrieve regions View EC2 instances Manage key pairs Acquired the ability to manage AWS resources using both Console \u0026amp; CLI.\nReflection \u0026amp; Plan Difficulties faced:\nMany new concepts CLI commands were confusing VPC concepts still abstract Next Week Goals (Week 2):\nIAM: Users, Groups, Policies Custom VPC + Subnet + Route table Practice: S3, EC2 automation via CLI Learn Bash Script + AWS CLI integration End of Week 1 Report\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Studied AWS VPC , VPN, DirectConnect, LoadBalancer; Subnets, Route Tables, Internet Gateway, NAT Gateway, Security Groups, Network ACLs, and connecting EC2 instances\nWeek 3: Configure Hybrid DNS, VPC Peering, Transit Gateway, and manage EC2 instances\nWeek 4: AWS S3, Backup \u0026amp; Storage Gateway\nWeek 5: Explore AWS security services, IAM, Cognito, KMS, Security Hub, and Lambda cost optimization\nWeek 6: Database Concepts, Amazon RDS \u0026amp; Aurora, Redshift, ElastiCache, DynamoDB, Data Lake, AWS Glue \u0026amp; Athena, QuickSight\nWeek 7: Define project scope \u0026amp; architecture, research serverless services, draft proposal\nWeek 8: Write project proposal, estimate cost using AWS Pricing Calculator, finalize serverless architecture\nWeek 9: Learn API fundamentals, REST API, HTTP methods, start backend project\nWeek 10: Develop backend Module: Cart, test API with Swagger\nWeek 11: Develop backend Modules: Order \u0026amp; Coupon, test full flow with Swagger\nWeek 12: Finalize all backend modules, fix bugs, test all APIs, handover to frontend\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"SAP Cloud ERP and SAP BTP Now Available on AWS Marketplace By Lynn Jones, Arne Knoeller, and Soumya Sekhar Das – 20 AUG 2025\nThe enterprise resource planning (ERP) landscape is experiencing a transformational shift, moving towards cloud-native solutions and a simpler approach to buying, deploying, and managing these solutions with faster speed and higher security.\nLeading this transformation, SAP and Amazon Web Services (AWS) continue to strengthen their strategic partnership, helping organizations of all sizes innovate faster and run critical business processes in the cloud.\n1. Launch History on AWS Marketplace AWS re:Invent 2024: SAP Cloud ERP (formerly GROW with SAP) launched on AWS Marketplace. SAP Sapphire 2025: SAP and AWS introduced SAP Business Technology Platform (BTP) Services on the Marketplace. SAP launched the SAP Business Suite, which includes: SAP Cloud ERP (GROW) Private (RISE) Other SAP services GROW and RISE evolved from standalone products into guided implementation frameworks or “journeys,” providing structured deployment paths for Cloud ERP and other Business Suite products.\n2. Benefits of Purchasing SAP Business Suite and SAP BTP via AWS Marketplace 2.1. Simplified Procurement Process Manage everything through your existing AWS account: one invoice, one set of terms, and a centralized management platform. Existing AWS spend commitments can be applied toward SAP purchases to maximize technology budget. Apply AWS commitments to Private Pricing spend commitment (PPA) → simplifies budget management. Procurement time is reduced from weeks to days, allowing teams to focus on running the business. 2.2. Trusted Guidance through Partners SAP \u0026amp; AWS partners support deployment of S/4HANA Cloud Edition:\nCognitus, Navigator, NTT Data, Syntax, Seidor, Navisite, AnswerThink, Bramasol, Vision33, VistaVu More partners will be added as the program expands 3. Benefits of SAP Business Suite and SAP BTP 3.1. Rapid Deployment of SAP Cloud ERP Time-to-value is 40–60% faster than on-premises ERP. Faster rollout and go-live using prebuilt processes, guided tools, and best practice methodologies from SAP and AWS. Includes: Embedded AI agents for intelligent automation Pre-configured industry best practices AWS infrastructure optimization, including Graviton processors Seamless integration with SAP BTP 3.2. Integration Capabilities Easily extendable with over 75 SAP BTP services on AWS and more than 200 AWS services. Leverages AWS global backbone network → lower latency, higher security, better performance. Traffic stays within AWS network → enhanced security posture, compliance with strict regulations. 3.3. Low/No-Code Extension Tools Customers can leverage: 750+ prebuilt integrations 700+ open APIs 6000+ reusable CDS views Build custom extensions with minimal coding or integration effort. 3.4. Modernization with AWS \u0026amp; SAP GenAI SAP Cloud ERP simplifies business operations and serves users better. Build generative AI applications securely with Amazon Bedrock (via SAP generative AI hub). Supports Joule, SAP’s generative AI copilot, across all SAP applications. 3.5. Scalability and Performance Provides scalability, high availability, and security. Supports regional localization. Leverages AWS innovations → best-in-class CPU price performance and energy efficiency. 4. Innovating with SAP Cloud ERP, BTP \u0026amp; AWS SAP BTP enables side-by-side extensions while maintaining a clean core in S/4HANA. Developers can choose languages: ABAP, Java, Node.js, Python. Cloud Foundry: supports Java, Node.js, Python Kyma: serverless functions (Node.js, Python), cross-platform microservices ABAP Cloud uses RAP, Java with Spring Boot, JavaScript with CAP Maximizes existing resources while choosing the most suitable tech stack per use case. 5. Use Cases – Innovation Examples 5.1. Intelligent Document Processing Amazon Textract automates data extraction from supplier invoices (Accounts Payable). Integrates Amazon Comprehend → entity recognition, sentiment analysis. Data is written back to S/4HANA via OData APIs. Deployable as an extension on SAP BTP, fully integrated with SAP workflows. 5.2. Data-to-Value – Advanced Analytics Export S/4HANA data to Amazon Redshift for deep analytics. Use AWS Glue for ETL and real-time synchronization. Java developers → Spring Boot, ABAP developers → AWS SDK for ABAP. 6. Overall Benefits Technical flexibility: add or deploy new functionality without affecting the core system. Cloud-native → natural scalability, modular expansion. Easy maintenance, clean core compliance → smooth S/4HANA upgrades. Pay-per-use pricing for BTP \u0026amp; AWS. Focus on business value, reduce technical debt. 7. Summary The availability of SAP Cloud ERP Business Suite and SAP BTP on AWS Marketplace marks a key milestone in enterprise cloud solutions:\nCombines SAP’s industry-leading ERP capabilities with AWS’s robust infrastructure. Access to 200+ AWS services and LLMs via Amazon Bedrock. Comprehensive, secure, scalable, and innovative solution. Simplifies purchasing, governance, and accelerates digital transformation. Recommendation: Contact AWS and AWS for SAP Partners to evaluate your IT landscape and determine where SAP Cloud ERP fits best.\nFurther Reading:\nSAP Business Suite Packages – AWS Marketplace SAP BTP – AWS Marketplace (AWS login required) "},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Multi-Rack and Multiple Logical AWS Outposts Architecture Considerations for Resiliency By Brianna Rosentrater – 20 AUG 2025\nTopics: Advanced (300), AWS Outposts, AWS Outposts rack, Best Practices\nAWS Outposts racks provide the same AWS infrastructure, services, APIs, and tools to nearly any on-premises data center or colocation space, delivering a truly consistent hybrid experience.\nA logical Outpost (hereafter called Outpost) is a deployment consisting of one or more Outposts racks physically connected and managed as a single entity under one Amazon Resource Name (ARN).\nAn Outpost provides a pool of AWS compute and storage capacity at one of your sites, acting as a private extension of an Availability Zone (AZ) in an AWS Region.\nMany AWS services supporting Outposts offer deployment options to improve fault tolerance for your workloads. However, these require certain Outposts configuration prerequisites.\nThis article explores architectural considerations when deciding between using a multi-rack logical Outpost or multiple independent Outposts racks to support highly available workloads.\nAmazon EC2 on AWS Outposts Rack Multi-Rack Logical Outposts With a multi-rack logical Outpost, you can use rack-level spread Amazon EC2 placement groups. A rack-level spread placement group can have as many partitions as the number of racks in your Outpost deployment, distributing instances to improve fault tolerance for workloads.\nFor example, consider C5 instances in an Amazon EC2 Auto Scaling group using a launch template with a rack-level spread placement group. If the multi-rack Outpost has four racks, instances are distributed evenly across all four racks.\nFigure 1: Rack-level spread Amazon EC2 placement group\nThis placement strategy improves resiliency against rack or host failures but does not protect against AZ failures.\nEC2 instances on Outposts remain stable against network disconnects, so workloads continue running during an AZ failure, though mutating actions are unavailable.\nMultiple Outposts Racks If you have more than one logical Outpost in a Region, connect each Outpost to a separate AZ. This enables multi-AZ resilient architectures, and when combined with features like Intra-VPC communication, you can extend an EC2 Auto Scaling group across multiple Outposts in the same VPC.\nFor single-rack Outposts, use host-level spread placement groups in your instance launch template. A host-level spread placement group can have partitions equal to the number of hosts for that instance type, improving resiliency against host failures.\nFor maximum resiliency, consider using multiple multi-rack logical Outposts. This allows rack-level spread placement groups and intra-VPC communication between Outposts.\nFigure 2: Intra-VPC communication between two multi-rack logical Outposts using Amazon EC2 Auto Scaling with rack-level spread\nAmazon RDS on AWS Outposts Rack Multi-Rack Logical Outposts Amazon RDS on Outposts supports read replicas, using asynchronous replication in MySQL and PostgreSQL engines. Read replicas can be placed within the same Outpost or another Outpost in the same VPC.\nSupports scaling read-heavy workloads beyond a single database instance. Can maintain a secondary copy for host failure resiliency. Promoting a read replica to primary must be manually initiated, updating DNS records. Multiple read replicas can be created to improve database resiliency, and multi-rack Outposts allow distribution across racks. Figure 3: Amazon RDS read replicas using a multi-rack Outpost\nMultiple Outposts Racks Multi-AZ Amazon RDS deployments are supported for MySQL and PostgreSQL on Outposts.\nUses Outposts Local Gateway and synchronous replication. A primary database instance runs on one Outpost, with a standby on another. Failover occurs automatically, with DNS updates. Protects against AZ failure, host failure, and Outpost failure. Can combine with read replicas across hosts or racks for added durability. Figure 4: Multi-AZ Amazon RDS on Outposts using read replicas for enhanced durability\nAmazon EKS on Outposts Rack Multi-Rack Logical Outposts Outposts racks support two EKS deployment models:\nEKS extended cluster EKS local cluster Using rack-level placement group strategy, you can distribute EKS instances (worker and control plane, depending on deployment) across multiple racks.\nControl plane instances are automatically replaced during instance, host, or rack failure. Self-managed worker nodes usually run in EC2 Auto Scaling groups. This combination improves resiliency and leverages automation for failure handling.\nFigure 5: EKS local cluster with rack-level spread placement group and auto scaling\nMultiple Outposts Racks EKS control plane instances cannot span multiple Outposts. For EKS local clusters, use an external load balancer and deploy clusters on each Outpost (active/active or active/passive). Consider persistent storage access requirements; use centralized storage or replication if needed. Two single-rack Outposts: distribute worker nodes across Outposts and use host-level spread on primary Outpost for control plane resiliency. Two multi-rack Outposts: use rack-level spread within primary multi-rack Outpost for control plane distribution. Figure 6: EKS local cluster using two multi-rack Outposts with rack-level spread\nAmazon S3 on Outposts Rack Multi-Rack Logical Outposts Supports object replication within the same Outpost or across Outposts. Multi-rack Outposts allow replicating objects to another bucket in the same Outpost for local resiliency. Figure 7: Amazon S3 replication between buckets on the same Outpost\nMultiple Outposts Racks Replicate S3 objects between buckets on different Outposts, each connected to a separate AZ for multi-AZ resiliency. Combine with local replication for high availability. AWS DataSync supports replication to the connected Region or for cloud-based tiering. Figure 8: Amazon S3 replication across two multi-rack Outposts\nFurther Considerations Connect each Outpost to a separate AZ for multi-AZ deployment options. Outposts are network-dependent; backup service links are recommended for high availability. Resources are finite; capacity can be adjusted dynamically with Capacity Tasks. References:\nAWS Direct Connect Resiliency Recommendations Satellite Resiliency for AWS Outposts Dynamically reconfigure your AWS Outposts capacity using Capacity Tasks Conclusion This article reviewed architecture options and considerations when choosing between multi-rack Outposts or multiple Outposts for high availability workloads.\nFor more guidance, see:\nAWS Outposts High Availability Design and Architecture Considerations Contact your AWS account team or fill out the form to learn more about Outposts and self-service capacity management.\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Report: “DevOps on AWS” Purpose of the Event Understand the fundamentals of DevOps and DevOps culture in modern organizations. Learn about CI/CD, IaC, and common DevOps tools. Explore how to implement DevOps practices on AWS using CloudFormation, CDK, and Amplify. Gain real-world insights from AWS community speakers and engineers. Speakers Truong Quang Tinh – AWS Community Builder, Platform Engineer – Timex Hoang Kha – DevOps Engineer Nguyen Bao – Cloud Engineer Nguyen Thinh – AWS Developer / CDK Specialist Key Content Overview 1. Building a DevOps Culture @Speaker: Truong Quang Tinh\nDev – DevOps – Operations Dev: develops code and features. Operations: deployment, system monitoring, maintenance. DevOps: combines Dev + Ops to increase delivery speed and reduce failures. Core Elements of DevOps Culture Collaboration \u0026amp; Shared Responsibility Automate Everything – testing, deployment, infrastructure Continuous Learning \u0026amp; Experimentation Measurement – using clear metrics and KPIs Next-Generation DevOps DevOps Cloud Platform Engineering Site Reliability Engineering (SRE) DevOps Success Metrics Deployment health monitoring Improved agility Faster delivery cycle Reduced deployment failures 2. DevOps Practices \u0026amp; CI/CD Pipeline @Speaker: Hoang Kha\nCI/CD Pipeline Overview of a standard CI/CD pipeline. Implementing pipelines using GitHub Workflow. Main Topics Cost optimization for DevOps pipelines. Applying open-source tools to reduce cost. Practical DevOps workflows in real-world projects. Continuous Integration (CI) Organizing repositories and team workflows. Testing at every stage of the pipeline. Common reasons for build failures: Coding errors Missing dependencies Failed test cases Merge conflicts Continuous Delivery (CD) Automating deployment from staging to production. Best Practices Proper branching \u0026amp; merging strategies Pull request workflows Naming conventions for branches Organizing source code effectively Modularizing containers 3. Infrastructure as Code (IaC) @Speaker: Nguyen Bao\nWhat is IaC? Manual console clicks (“ClickOps”) → error-prone and not scalable. IaC uses code to define and manage infrastructure. AWS CloudFormation AWS-native IaC service. Stack: group of resources defined in one template. Template: YAML file describing the resources. CloudFormation Workflow Template → Deployment → Stack creation Templates can be version-controlled in Git repositories. Configuration Drift Occurs when the actual infrastructure differs from the template. CloudFormation provides Drift Detection to identify inconsistencies. 4. AWS CDK (Cloud Development Kit) @Speaker: Nguyen Thinh\nCDK Components Constructs – the fundamental building blocks\nL1 Constructs – raw CloudFormation level L2 Constructs – opinionated, provide defaults L3 Constructs – complete patterns (e.g., API + Lambda + DynamoDB) App – root construct\nStack – collection of resources\nAdvantages of CDK Write infrastructure using real programming languages (TypeScript, Python…) Easier to read, reuse, and maintain. AWS Amplify Built on top of CloudFormation \u0026amp; CDK. Each environment (production, sandbox) corresponds to a separate stack. Important Note Avoid circular dependencies when using the AWS SDK inside templates. 5. Choosing Between IaC Tools @Speaker: Nguyen Bao\nComparison of IaC Tools Terraform OpenTofu Pulumi Each has different strengths depending on project requirements.\nKey Takeaways DevOps Knowledge \u0026amp; Mindset Understand DevOps culture and how to apply it in organizations. Learn standard CI/CD processes and best-practice workflows. Know how to choose IaC tools for different use cases. AWS Knowledge Manage infrastructure with CloudFormation and CDK. Understand stacks, templates, and drift detection. Learn how Amplify manages frontend environments. Real-World Experience Optimize DevOps pipeline costs. Use open-source tools to reduce expenses. Apply best practices for code organization and repository structure. Lessons Learned IaC is essential for scaling infrastructure. Automation is the foundation of modern DevOps. DevOps is not just tools → it\u0026rsquo;s a culture and working mindset. CDK makes infrastructure reusable, maintainable, and extendable. "},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":"IAM Permissions Create an IAM role with Permissions policies: AmazonBedrockFullAccess and CloudWatchLogsFullAccess\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/2-proposal/","title":"Proposal","tags":[],"description":"","content":"\u0026ldquo;Leaf\u0026rdquo; Clothing E-commerce Website Using AWS Services Integration to Optimize the System (S3, CloudWatch, etc.) 1. Executive Summary Leaf is an e-commerce platform specializing in fashion products for men and women, including fashion accessories such as jewelry, shoes, hats, and leather straps. The website integrates AWS services to optimize costs and enhance the user experience.\n2. Problem Statement Problem:\nA clothing store needs its own e-commerce channel to optimize the user experience.\nSolution:\nCreate a dedicated e-commerce website for the store using AWS services to optimize cost, time, and user experience.\n3. Solution Architecture The platform applies an AWS Serverless architecture to manage data.\nComponents and Roles in the AWS Architecture A. User Interface Layer Service Role Detailed Description AWS Amplify Website deployment Hosts static websites (React, Vue, Next.js) and automatically builds/deploys when code is pushed to GitHub. Amazon CloudFront (CDN) Improve page loading speed Caches static content (images, CSS, JS) close to users to reduce latency and bandwidth usage. Amazon S3 Store static files \u0026amp; product images Acts as a content repository for images, banners, CSS/JS files. B. Application Logic Layer Service Role Detailed Description Amazon API Gateway API gateway Receives requests from frontend and forwards them to Lambda functions for processing. AWS Lambda Server-side logic Handles order processing, payments, authentication, email sending, without dedicated servers. Amazon DynamoDB NoSQL database Stores products, accounts, orders, shopping carts, offering high speed and automatic scaling. Amazon OpenSearch Service Product search Allows users to quickly search products by keywords, color, price, etc. Amazon EventBridge Event handling Automatically triggers events (e.g., new order → send email, update stock). AWS Secrets Manager Secure sensitive data Stores API keys, payment tokens, database passwords to protect data. C. User Management \u0026amp; Security Layer Service Role Detailed Description Amazon Cognito User authentication \u0026amp; management Supports signup, login, password reset, MFA without building your own auth system. AWS WAF (Web Application Firewall) Web protection Protects against SQL Injection, XSS, DDoS, and other malicious access. Amazon Route 53 DNS \u0026amp; domain Manages domain names. D. Notification \u0026amp; Communication Layer Service Role Detailed Description Amazon SNS (Simple Notification Service) System notifications Sends notifications to admins or users (via email, SMS, or push notifications). Amazon SES (Simple Email Service) Transactional emails Sends order confirmations, promotions, password reset emails, etc. E. AI \u0026amp; Machine Learning Layer Service Role Detailed Description Amazon Translate Content translation Translates product descriptions to other languages for international customers. Amazon Bedrock AI content generation Creates chatbots for shopping assistance. F. Monitoring \u0026amp; Management Layer Service Role Detailed Description Amazon CloudWatch System monitoring Monitors logs, performance, alerts for errors or cost spikes. AWS CloudTrail Administrative logging Tracks configuration changes (who changed what, and when) for auditing purposes. 4. Technical Implementation Implementation Stages:\nCollect system requirements and features Estimate cost and check feasibility Design UI prototypes using Figma Build database schema Develop frontend interface Build API, backend, integrate AWS services Test, deploy, and finalize the project 5. Timeline \u0026amp; Milestones Month 1: Learn AWS Month 2: Design and implement the project Month 3: Deployment and testing 6. Budget Estimate Check cost here: AWS Pricing Calculator\nStorage \u0026amp; Data Services Service Function Estimated Usage Cost/Month (USD) Notes Amazon S3 Store images, CSS, JS 10 GB storage, ~5k GET, ~500 PUT 0.35 Low data, low traffic DynamoDB Store orders \u0026amp; carts ~1 GB data, 100k read/write 0.20 On-demand mode OpenSearch Service Product search 1 small instance, 10% uptime 3.00 Reduced configuration due to small dataset Backend \u0026amp; Logic Processing Service Function Estimated Usage Cost/Month (USD) Notes AWS Lambda API processing, payments 100k requests, 128MB, 100ms 0.20 Very cheap due to serverless API Gateway API access 100k requests 0.10 Directly connected to Lambda Secrets Manager Secure API keys, DB 1 secret 0.40 Maintained EventBridge Internal event triggers 1k events/month 0.05 Lightweight for notifications/orders User Interface, Authentication \u0026amp; Security Service Function Estimated Usage Cost/Month (USD) Notes Amplify Hosting Frontend deployment 10 GB, 3 builds/month 1.50 CI/CD + static hosting CloudFront (CDN) Content delivery 5 GB out 0.20 Reduce CDN cost WAF (Web Firewall) Web protection 1 ACL 5.00 Basic security required Cognito User authentication 100 MAU 1.00 Reduced from $5 baseline Route 53 Domain DNS 1 hosted zone 0.50 Unchanged Email \u0026amp; Notifications Service Function Estimated Usage Cost/Month (USD) Notes SES (Email) Order confirmation emails 1,000 emails/month 0.15 $0.0001 per email SNS (Notifications) HTTP/email notifications 1k messages 0.05 Used for internal notifications AI \u0026amp; Machine Learning (Optional) Service Function Estimated Usage Cost/Month (USD) Notes Translate Product translation EN↔VI 10k characters 0.15 Support international customers Bedrock Generate product descriptions 100 small requests 0.10 Can be disabled if not needed Monitoring \u0026amp; Logging Service Function Estimated Usage Cost/Month (USD) Notes CloudWatch Logs, metrics monitoring 1–2 GB log 1.50 Reduced from $9 CloudTrail Activity auditing Default usage 0.00 Free tier sufficient Total Estimated Monthly Cost Service Group Total Cost (USD/month) Storage \u0026amp; Data 3.55 Backend \u0026amp; Processing 0.75 UI \u0026amp; Security 8.20 Email \u0026amp; Notifications 0.20 AI \u0026amp; ML (Optional) 0.25 Monitoring \u0026amp; Logs 1.50 Total (Actual) ≈ 14.45 USD / month (~375,000 VND) 7. Expected Outcomes The website runs with low latency and smooth image display. "},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Getting started with healthcare data lakes: Using microservices Guides building a healthcare data lake using microservices, helping to store and analyze diverse healthcare data (electronic medical records, lab tests, IoT devices).\nKey points: environment setup, data pipeline construction, security \u0026amp; HIPAA compliance, deployment and scaling.\nBlog 2 - SAP Cloud ERP and SAP BTP on AWS Marketplace Introduces deploying SAP Cloud ERP and SAP BTP via AWS Marketplace, enabling enterprises to deploy quickly, securely, and integrate with AWS services.\nKey points: AWS Marketplace benefits, ERP deployment, BTP usage, AWS service integration, real-world use cases (IDP, Advanced Analytics).\nBlog 3 - Multi-Rack and Multiple Logical AWS Outposts: Architecture for Resiliency Explains designing AWS Outposts to enhance workload availability and resiliency. Covers: EC2 with rack-level and host-level spread placement groups, multi-AZ deployments across multiple Outposts; RDS with read replicas and multi-AZ setups for data durability; EKS distributing worker nodes and control plane, considering persistent storage; S3 replication within the same Outpost or across multiple Outposts, combined with DataSync for backup and cloud tiering.\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/4-eventparticipated/","title":"Events Attended","tags":[],"description":"","content":"During my internship, I had the opportunity to participate in two major AWS community events.\nEach event provided valuable knowledge, practical insights, and memorable experiences.\nEvent 1 Event Name: Generative AI with Amazon Bedrock\nTime: 09:00 – November 21, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nSummary:\nThe event provided foundational knowledge about Generative AI, Amazon Bedrock, Prompt Engineering techniques, RAG, and various pre-trained AI services from AWS. I also learned how to build agentic AI applications using Bedrock AgentCore. This event helped me better understand how GenAI can be applied to real-world business use cases, especially in chatbot development and information retrieval.\nEvent 2 Event Name: DevOps on AWS\nTime: 14:00 – November 21, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nSummary:\nThe event offered a comprehensive overview of DevOps, DevOps culture, CI/CD, IaC, and tools such as GitHub Workflow, CloudFormation, CDK, and Amplify. I also gained insights into cost optimization for DevOps pipelines, the importance of automation, and best practices for building pipelines and organizing source code effectively.\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/5-workshop/5.4-setup_lambda/","title":"Setup Lambda","tags":[],"description":"","content":"1. Go to Lambda You can code with Lambda using languages as shown in the image. In this workshop, we will use Node.js 2. Click on Create function Fill in the function name and select the runtime type Click on \u0026ldquo;Change default execution role\u0026rdquo;, select \u0026ldquo;existing role\u0026rdquo;, then click on the role you created earlier for the workshop Click on \u0026ldquo;Create function\u0026rdquo; 3. Set up function\nGo to the Configuration tab Increase memory to 500MB and timeout to 2 minutes Go to the Code tab and paste this code: Click the Deploy button const { BedrockRuntimeClient, InvokeModelCommand } = require(\u0026#34;@aws-sdk/client-bedrock-runtime\u0026#34;); const client = new BedrockRuntimeClient({ region: \u0026#34;us-east-1\u0026#34; }); const CORS_HEADERS = { \u0026#34;Access-Control-Allow-Origin\u0026#34;: process.env.ALLOWED_ORIGIN || \u0026#34;*\u0026#34;, \u0026#34;Access-Control-Allow-Headers\u0026#34;: \u0026#34;Content-Type,X-Amz-Date,Authorization,X-Api-Key\u0026#34;, \u0026#34;Access-Control-Allow-Methods\u0026#34;: \u0026#34;OPTIONS,POST\u0026#34;, \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; }; const MAX_MESSAGE_LENGTH = 2000; const MAX_HISTORY_TURNS = 10; exports.handler = async (event) =\u0026gt; { const startTime = Date.now(); console.log(\u0026#34;Request received:\u0026#34;, { sourceIp: event.requestContext?.identity?.sourceIp || event.requestContext?.http?.sourceIp, userAgent: event.requestContext?.identity?.userAgent || event.headers?.[\u0026#34;user-agent\u0026#34;] }); const httpMethod = event.httpMethod || event.requestContext?.http?.method; if (httpMethod === \u0026#34;OPTIONS\u0026#34;) { return { statusCode: 200, headers: CORS_HEADERS, body: \u0026#34;\u0026#34; }; } let body; try { body = JSON.parse(event.body || \u0026#34;{}\u0026#34;); } catch (e) { console.error(\u0026#34;Invalid JSON:\u0026#34;, e.message); return { statusCode: 400, headers: CORS_HEADERS, body: JSON.stringify({ error: \u0026#34;Invalid JSON format\u0026#34; }) }; } const userMessage = (body.message || \u0026#34;\u0026#34;).toString().trim(); const history = Array.isArray(body.history) ? body.history : []; if (!userMessage) { return { statusCode: 400, headers: CORS_HEADERS, body: JSON.stringify({ error: \u0026#34;Message is required\u0026#34; }) }; } if (userMessage.length \u0026gt; MAX_MESSAGE_LENGTH) { return { statusCode: 400, headers: CORS_HEADERS, body: JSON.stringify({ error: `Message too long. Maximum ${MAX_MESSAGE_LENGTH} characters allowed.` }) }; } // Claude uses messages format const messages = []; const recentHistory = history.slice(-MAX_HISTORY_TURNS); for (const turn of recentHistory) { if (turn.user) { messages.push({ role: \u0026#34;user\u0026#34;, content: turn.user }); } if (turn.assistant) { messages.push({ role: \u0026#34;assistant\u0026#34;, content: turn.assistant }); } } messages.push({ role: \u0026#34;user\u0026#34;, content: userMessage }); const requestBody = { anthropic_version: \u0026#34;bedrock-2023-05-31\u0026#34;, max_tokens: 512, messages: messages, temperature: 0.7 }; const modelId = \u0026#34;arn:aws:bedrock:us-east-1:756859458422:inference-profile/us.anthropic.claude-haiku-4-5-20251001-v1:0\u0026#34;; try { console.log(\u0026#34;Invoking Bedrock model:\u0026#34;, modelId); const command = new InvokeModelCommand({ modelId, contentType: \u0026#34;application/json\u0026#34;, accept: \u0026#34;application/json\u0026#34;, body: JSON.stringify(requestBody) }); const response = await client.send(command); const result = JSON.parse(new TextDecoder().decode(response.body)); const reply = result.content[0].text || \u0026#34;Sorry, I cannot generate a response.\u0026#34;; const duration = Date.now() - startTime; console.log(\u0026#34;Request completed:\u0026#34;, { duration, responseLength: reply.length }); return { statusCode: 200, headers: CORS_HEADERS, body: JSON.stringify({ response: reply }) }; } catch (error) { console.error(\u0026#34;Bedrock invocation error:\u0026#34;, { message: error.message, code: error.code, modelId: modelId }); return { statusCode: 500, headers: CORS_HEADERS, body: JSON.stringify({ error: \u0026#34;AI service is temporarily unavailable. Please try again later.\u0026#34; }) }; } }; ``` **Related Documentation:** https://docs.aws.amazon.com/lambda/latest/dg/getting-started.html "},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/5-workshop/5.5-setup-api-gateway/","title":"Setup API Gateway","tags":[],"description":"","content":"1. Go to API Gateway\nClick \u0026ldquo;Create an API\u0026rdquo; Select \u0026ldquo;Build\u0026rdquo; under REST API Enter a name Choose a security policy Click \u0026ldquo;Create API\u0026rdquo; 2. Create Resource\nClick \u0026ldquo;Create Resource\u0026rdquo; Name the resource and enable CORS Click the \u0026ldquo;Create Resource\u0026rdquo; button 3. Create Method\nClick \u0026ldquo;Create Method\u0026rdquo; Select POST and enable Lambda Proxy Integration Add the Lambda function ARN Click \u0026ldquo;Create Method\u0026rdquo; 4. Deploy API\nClick \u0026ldquo;Deploy API\u0026rdquo; Choose \u0026ldquo;New Stage\u0026rdquo; and name the stage Click the \u0026ldquo;Deploy\u0026rdquo; button 5. Test\nAfter creating the stage, you will get the Invoke URL Use the Invoke URL to test in Postman with the format InvokeURL/, using the POST method Use the following JSON body:\n{ \u0026ldquo;message\u0026rdquo;: \u0026ldquo;What is Amazon Bedrock? Please answer briefly, no more than 3 lines.\u0026rdquo; }\nClick \u0026ldquo;Send\u0026rdquo; You should receive a successful response\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Overview Amazon Bedrock provides the ability to integrate leading foundation models (LLMs) from Anthropic, Meta, AI21 Labs, and other providers through a simple API, allowing you to build AI applications without managing complex infrastructure.\nIn this workshop, we will learn how to build, deploy, and test a complete AI Chatbot using a serverless architecture, enabling users to interact with Claude Haiku 4.5 without having to manage servers or worry about scaling.\nWe will create a system with five main components to build the chatbot: Amazon Bedrock (AI engine), AWS Lambda (backend logic), API Gateway (REST API endpoint), Amazon S3 (frontend hosting), and CloudWatch (monitoring). These components deliver a fully serverless architecture with low cost and automatic scaling.\nMain Components: Amazon Bedrock (AI Engine) – Provides the Claude Haiku 4.5 model through a simple API. You call the InvokeModel API to send messages and receive intelligent AI responses.\nAWS Lambda (Backend Logic) – Runs Node.js 24 code to process requests from API Gateway. Lambda validates input (message length, history limits), formats prompts for Bedrock, calls the Bedrock API, and handles errors.\nAPI Gateway (REST API Endpoint) – Creates a public HTTPS endpoint (POST /chat) for the frontend to call. API Gateway handles CORS configuration, routes requests to Lambda, and provides throttling to protect the backend from abuse.\nAmazon S3 (Frontend Hosting) – Hosts a static website (HTML) for the chatbot UI. Users access the chatbot through the browser; the interface sends messages to API Gateway and displays AI responses.\nCloudWatch (Monitoring \u0026amp; Debugging) – Automatically collects logs from Lambda execution, allowing you to debug errors.\nArchitecture Overview User Browser → S3 (Static Website) → API Gateway → Lambda → Bedrock (Claude) → CloudWatch\nFlow:\nUser enters a message in the chatbot UI (hosted on S3) JavaScript sends a POST request to the API Gateway endpoint API Gateway invokes the Lambda function Lambda validates input, formats the prompt, and calls the Bedrock InvokeModel API Bedrock processes it with Claude Haiku 4.5 and returns the AI response Lambda returns the response back to API Gateway → Browser CloudWatch logs the entire process Workshop Modules Module 1: Setup Amazon Bedrock\n• Enable Claude Haiku 4.5 model access\n• Understand inference profiles\n• Test the model via AWS Console\nModule 2: Create Lambda Function\n• Create a Node.js 24 Lambda function\n• Deploy chatbot backend code\n• Configure IAM role with Bedrock permissions\n• Set environment variables\nModule 3: Configure API Gateway\n• Create REST API\n• Configure POST /chat endpoint\n• Enable CORS\n• Test API with Postman\nModule 4: Deploy Frontend to S3\n• Create S3 bucket\n• Enable static website hosting\n• Upload HTML chatbot UI\n• Configure public access\nModule 5: Testing \u0026amp; Debugging\n• Test end-to-end flow\n• View CloudWatch logs\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/5-workshop/5.6-monitoring-cloudwatch/","title":"Monitoring with CloudWatch","tags":[],"description":"","content":"1. In Lambda, go to the Monitor tab\nClick \u0026ldquo;View CloudWatch logs\u0026rdquo; You can view the function logs via Log Streams "},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/5-workshop/5.7-setup-s3/","title":"Prepare S3","tags":[],"description":"","content":"1. Create an HTML file to deploy to S3\nReplace InvokeURL/resourceName in the code below with your invoke URL. \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;/\u0026gt; \u0026lt;title\u0026gt;Chatbot - ChatGPT Style\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { margin: 0; font-family: \u0026#34;Inter\u0026#34;, \u0026#34;Segoe UI\u0026#34;, sans-serif; background-color: #f0f0f0; display: flex; flex-direction: column; height: 100vh; } header { background-color: #2e7d32; /* Green */ color: white; padding: 1rem; text-align: center; font-weight: bold; font-size: 1.2rem; } #chatBox { flex: 1; overflow-y: auto; padding: 1.5rem; display: flex; flex-direction: column; gap: 1rem; } /* ChatGPT-like message blocks */ .message { width: 100%; max-width: 800px; margin: auto; padding: 1rem; border-radius: 10px; font-size: 1rem; line-height: 1.5; border: 1px solid #ddd; background-color: #ffffff; } .user { background-color: #e8f5e9; border-color: #c8e6c9; } .bot { background-color: #ffffff; border-color: #e0e0e0; } footer { padding: 1rem; background-color: #f7f7f8; border-top: 1px solid #ddd; display: flex; justify-content: center; } .input-container { width: 100%; max-width: 800px; display: flex; gap: 0.5rem; background: white; border: 1px solid #ccc; padding: 0.75rem; border-radius: 12px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); } input { flex: 1; border: none; outline: none; font-size: 1rem; background: none; } button { background-color: #4caf50; color: white; border: none; padding: 0.6rem 1rem; font-size: 1rem; border-radius: 8px; cursor: pointer; transition: 0.2s ease; } button:hover { background-color: #449d48; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;header\u0026gt;🌿 Chatbot \u0026lt;/header\u0026gt; \u0026lt;div id=\u0026#34;chatBox\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;footer\u0026gt; \u0026lt;div class=\u0026#34;input-container\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;userInput\u0026#34; placeholder=\u0026#34;Send a message...\u0026#34; /\u0026gt; \u0026lt;button onclick=\u0026#34;sendMessage()\u0026#34;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;script\u0026gt; let history = []; async function sendMessage() { const userInput = document.getElementById(\u0026#34;userInput\u0026#34;); const message = userInput.value.trim(); if (!message) return; addMessage(message, \u0026#34;user\u0026#34;); const response = await fetch(\u0026#34;InvokeURL/resourceName\u0026#34;, { method: \u0026#34;POST\u0026#34;, headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; }, body: JSON.stringify({ message, history }) }); const data = await response.json(); const botReply = data.response; addMessage(botReply, \u0026#34;bot\u0026#34;); history.push({ user: message, assistant: botReply }); userInput.value = \u0026#34;\u0026#34;; } function addMessage(text, sender) { const msg = document.createElement(\u0026#34;div\u0026#34;); msg.classList.add(\u0026#34;message\u0026#34;, sender); msg.textContent = text; document.getElementById(\u0026#34;chatBox\u0026#34;).appendChild(msg); msg.scrollIntoView({ behavior: \u0026#34;smooth\u0026#34; }); } \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 2. Go to S3\nClick Create bucket\nEnter a bucket name\nDisable Block public access\n(While the best practice is keeping the bucket private, for this workshop we set it to public for easier testing.)\nClick Create bucket\n3. Open the bucket you just created\nGo to the Properties tab\nClick Edit under Static website hosting\nSelect Enable\nClick Save changes\nGo to the Permissions tab → Edit Bucket policy\nPaste the following policy (replace your-bucket-name with your actual bucket name):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::your-bucket-name/*\u0026#34; } ] } Click Save changes\nGo to the Objects tab and upload your HTML file\nClick the Object URL to open your deployed HTML page\nDeployment completed successfully\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/6-self-evaluation/","title":"Self-Evaluation","tags":[],"description":"","content":"During my internship at [AWS] from August 9, 2025 to December 24, 2025, I had the opportunity to learn, practice, and apply the knowledge I acquired at university in a real working environment.\nIn terms of work ethic, I always strived to complete my tasks responsibly, follow all internal regulations, and actively communicate with teammates to improve work efficiency.\nTo provide an objective reflection of my internship journey, I hereby present my self-evaluation based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional Knowledge \u0026amp; Skills Understanding of the field, ability to apply knowledge, tool usage skills, work quality ☐ ✅ ☐ 2 Ability to Learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Ability to research, take initiative, and accept tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of Responsibility Completing tasks on time and ensuring the expected quality ☐ ✅ ☐ 5 Discipline Compliance with working hours, regulations, and workflows ☐ ✅ ☐ 6 Willingness to Improve Openness to feedback and willingness to improve oneself ✅ ☐ ☐ 7 Communication Skills Ability to present ideas clearly and report work effectively ☐ ✅ ☐ 8 Teamwork Ability to collaborate effectively with colleagues and participate in group activities ✅ ☐ ☐ 9 Professional Conduct Respect for colleagues, partners, and the working environment ✅ ☐ ☐ 10 Problem-Solving Skills Ability to identify issues, propose solutions, and think creatively ☐ ✅ ☐ 11 Contribution to Projects/Team Work effectiveness, improvements, and recognition from the team ☐ ✅ ☐ 12 Overall Performance General assessment of the entire internship ☐ ✅ ☐ Areas for Improvement Improve discipline and strictly follow rules and policies of the company or any professional organization. Strengthen critical thinking and problem-solving skills. Enhance communication skills in daily interactions and workplace situations. "},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives Define the scope of the “Leaf” clothing e-commerce website. Create a system architecture idea using AWS. Research AWS services related to Serverless, Security, and Database. Start drafting the project proposal. Tasks Completed During the Week Day Task Start Date Completion Date Reference 2 Define project scope \u0026amp; requirements 2025-10-20 2025-10-20 CloudJourney 3 Research AWS Serverless \u0026amp; suitable services 2025-10-21 2025-10-21 CloudJourney 4 Design AWS architecture overview for the project 2025-10-22 2025-10-22 CloudJourney 5 Gather database \u0026amp; authentication requirements 2025-10-23 2025-10-23 CloudJourney 6 Draft the proposal outline 2025-10-24 2025-10-24 - 7 Review \u0026amp; refine proposal objectives 2025-10-25 2025-10-25 - Week 7 Achievements: This week focused mainly on research and laying the foundation for the “Leaf” project.\nKey insights gained:\nDefined core functionalities of the clothing e-commerce website.\nDesigned a suitable serverless architecture.\nIdentified AWS services required for each component:\nAmplify / CloudFront / S3 → Frontend hosting API Gateway + Lambda → Backend Logic DynamoDB → Database Cognito → User Authentication SES / SNS → Email \u0026amp; Notifications CloudWatch / CloudTrail → Monitoring \u0026amp; Logging End of Week 7 Report\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives Write the official proposal for the project. Estimate project cost using AWS Pricing Calculator. Finalize the serverless architecture on AWS. Tasks Completed During the Week Day Task Start Date Completion Date Reference Material 2 Finalized AWS architecture for the project 2025-10-27 2025-10-27 3 Designed the DynamoDB database tables 2025-10-28 2025-10-28 4 Used AWS Pricing Calculator to estimate project cost 2025-10-29 2025-10-29 5 Wrote the proposal draft 2025-10-30 2025-10-31 6 Refined and completed the proposal 2025-10-30 2025-10-31 Week 8 Achievements: Completed the full proposal. Generated an optimized AWS cost estimation using AWS Pricing Calculator. End of Week 8 Report\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives Learn what an API is and its role in a backend system. Understand REST API concepts, HTTP methods, and status codes. Start implementing the backend for the project: project structure, first module. Prepare the plan for Weeks 10–11. Tasks Completed During the Week Day Task Start Date Completion Date Reference 2 Learned the fundamentals of APIs 2025-11-03 2025-11-03 3 Studied REST API, HTTP methods, status codes 2025-11-04 2025-11-04 4 Researched API design: endpoints, request/response models, versioning 2025-11-05 2025-11-05 5 Initialized backend project (Spring Boot) \u0026amp; set up folder structure 2025-11-06 2025-11-06 6 Started coding the first backend module 2025-11-07 2025-11-07 Achievements Understood what an API is and why it is important. Gained knowledge about REST APIs and core HTTP methods: GET, POST, PUT, DELETE. Learned how to design clean and standard API endpoints. Successfully initialized the backend project and set up the full code structure. Implemented the first API module for the system. End of Week 9 Report\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives Study the structure of REST APIs in backend development. Start building the backend: Cart Module. Test APIs using Swagger UI. Tasks Completed During the Week Day Task Start Completed 2 Studied APIs and request–response flow 2025-11-10 2025-11-10 3 Designed the Cart data model 2025-11-11 2025-11-11 4 Implemented APIs: create cart, add item 2025-11-12 2025-11-12 5 Implemented APIs: update \u0026amp; remove cart items 2025-11-13 2025-11-13 6 Tested all Cart APIs using Swagger 2025-11-14 2025-11-14 Achievements Gained a solid understanding of how backend APIs work. Completed the Cart Module. Successfully connected the backend with DynamoDB. All APIs were tested thoroughly using Swagger UI. End of Week 10 Report\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/1-worklog/1.11-week11/","title":"Worklog Week 11","tags":[],"description":"","content":"Week 11 Goals Complete the Order and Coupon modules. Test backend APIs using Swagger. Tasks Completed During the Week Day Task Start Finish 2 Designed Order \u0026amp; Coupon tables 2025-11-17 2025-11-17 3 Implemented APIs: create \u0026amp; validate orders 2025-11-18 2025-11-18 4 Implemented payment processing APIs for orders 2025-11-19 2025-11-19 5 Implemented APIs for applying discount codes \u0026amp; validating coupons 2025-11-20 2025-11-20 6 Tested full flow in Swagger (Cart → Order → Coupon) 2025-11-21 2025-11-21 Achievements Completed the Order module. Completed the Coupon module. Successfully tested the entire flow using Swagger UI. End of Week 11 Report\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/1-worklog/1.12-week12/","title":"Worklog Week 12","tags":[],"description":"","content":"Week 12 Goals Complete all assigned backend tasks. Review code and fix bugs. Test all APIs using Swagger. Hand over the backend to the frontend team. Tasks Completed During the Week Day Task Start Finish 2 Reviewed Cart, Order, and Coupon modules 2025-11-24 2025-11-24 3 Fixed API issues 2025-11-25 2025-11-25 4 Tested the entire backend using Swagger UI 2025-11-26 2025-11-26 5 Wrote API documentation 2025-11-27 2025-11-27 6 Handed over the completed backend to the frontend team 2025-11-28 2025-11-28 Achievements Completed all assigned backend tasks. All APIs are stable and fully functional (Cart → Order → Coupon). Fully tested using Swagger UI. API documentation provided for smooth frontend integration. End of Week 12 Report\n"},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://sonnguyen181004.github.io/fcj-sonnt/tags/","title":"Tags","tags":[],"description":"","content":""}]